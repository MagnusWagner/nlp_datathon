{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use this notebook in colab because it can be too much for a standard GPU\n",
    "\n",
    "! pip install transformers\n",
    "! pip install datasets\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_path = \"/content/gdrive/My Drive/data/train_from0.csv\"\n",
    "test_path = \"/content/gdrive/My Drive/data/train_from0.csv\"\n",
    "sentence1_key = \"Narrative\"\n",
    "model_checkpoint = \"bert-base-german-cased\" # \"distilbert-base-uncased\"   bert-base-german-cased\n",
    "batch_size = 2\n",
    "num_labels = 6\n",
    "\n",
    "dataset = load_dataset('csv', data_files={\"train\": train_path, \"validation\": test_path})\n",
    "metric = load_metric(\"accuracy\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "if model_checkpoint == \"gpt2\":\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[sentence1_key], truncation=True, padding=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels =num_labels)\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"final\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model.is_parallelizable = False\n",
    "\n",
    "FeatureExtractionPipeline(model, tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "evaluation = trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"/content/gdrive/My Drive/data/final_model\")\n",
    "\n",
    "# later use the saved folder in the forward_app.ipynb to automatically classify and forward the emails"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-ebded89c",
   "language": "python",
   "display_name": "PyCharm (nlp_datathon)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}